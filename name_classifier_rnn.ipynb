{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "name-classifier-rnn",
      "provenance": [],
      "authorship_tag": "ABX9TyPD3DoUykTPyPKMeudWO8Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaybirrd/nlp_name_classifier/blob/master/name_classifier_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be8m1uB3-1bT",
        "colab_type": "text"
      },
      "source": [
        "# NLP FROM SCRATCH: CLASSIFYING NAMES WITH A CHARACTER-LEVEL RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwoZBn1d9lT1",
        "colab_type": "text"
      },
      "source": [
        "# clone data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrvfWIHy9kgl",
        "colab_type": "code",
        "outputId": "ab8ad616-9197-4bda-d6fe-8f9a36247646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/jaybirrd/nlp_name_classifier.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp_name_classifier'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 49 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POgB41g0iqaC",
        "colab_type": "text"
      },
      "source": [
        "# prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGNSSRugx_3B",
        "colab_type": "code",
        "outputId": "c75c1c7a-294d-43de-f578-7a1fdd91a58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def find_files(path):\n",
        "  return glob.glob(path)\n",
        "\n",
        "#print(find_files('nlp_name_classifier/data/names/*.txt'))\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "  )\n",
        "\n",
        "#print(unicode_to_ascii('Ślusàrski'))\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "def read_lines(filename):\n",
        "  lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "  return [unicode_to_ascii(line) for line in lines]\n",
        "\n",
        "#FUTURE DEV: put a popup here to select different datasets to analyze\n",
        "\n",
        "for filename in find_files('nlp_name_classifier/data/names/*.txt'):\n",
        "  category = os.path.splitext(os.path.basename(filename))[0]\n",
        "  all_categories.append(category)\n",
        "  lines = read_lines(filename)\n",
        "  category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "#print(category_lines['Italian'][:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nlp_name_classifier/data/names/Polish.txt', 'nlp_name_classifier/data/names/French.txt', 'nlp_name_classifier/data/names/Vietnamese.txt', 'nlp_name_classifier/data/names/Chinese.txt', 'nlp_name_classifier/data/names/German.txt', 'nlp_name_classifier/data/names/Scottish.txt', 'nlp_name_classifier/data/names/Portuguese.txt', 'nlp_name_classifier/data/names/Japanese.txt', 'nlp_name_classifier/data/names/English.txt', 'nlp_name_classifier/data/names/Korean.txt', 'nlp_name_classifier/data/names/Spanish.txt', 'nlp_name_classifier/data/names/Italian.txt', 'nlp_name_classifier/data/names/Irish.txt', 'nlp_name_classifier/data/names/Greek.txt', 'nlp_name_classifier/data/names/Czech.txt', 'nlp_name_classifier/data/names/Russian.txt', 'nlp_name_classifier/data/names/Arabic.txt', 'nlp_name_classifier/data/names/Dutch.txt']\n",
            "Slusarski\n",
            "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "126YpQL3G5Uy",
        "colab_type": "text"
      },
      "source": [
        "# turn names into tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZR2-ZRV8CBC",
        "colab_type": "code",
        "outputId": "496cf3ad-91aa-4422-cc64-0d375260243b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "def letter_to_index(letter):\n",
        "  return all_letters.find(letter)\n",
        "\n",
        "#turn a letter into a <1 x n_letters> Tensor\n",
        "def letter_to_tensor(letter):\n",
        "  tensor = torch.zeros(1,n_letters)\n",
        "  tensor[0][letter_to_index(letter)] = 1\n",
        "  return tensor\n",
        "\n",
        "#turn lines into a <line length x 1 x n_letters> Tensor\n",
        "def line_to_tensor(line):\n",
        "  tensor = torch.zeros(len(line),1,n_letters)\n",
        "  for i, letter in enumerate(line):\n",
        "    tensor[i][0][letter_to_index(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "#print(line_to_tensor('James').size())\n",
        "#print(letter_to_tensor('J'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 1, 57])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sws4a9Js-sYS",
        "colab_type": "text"
      },
      "source": [
        "# creating the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h0k_9ks-bxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}